# ğŸ”— RimAI Framework é›†æˆæŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜ RimAI Core å¦‚ä½•ä¸ RimAI Framework é›†æˆï¼ŒåŒ…æ‹¬æ¶æ„è®¾è®¡ã€è°ƒç”¨é“¾è·¯å’Œæœ€ä½³å®è·µã€‚

## ğŸ—ï¸ æ¶æ„è®¾è®¡å›¾

### æ•´ä½“æ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RimAI Core Layer                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  AIå®˜å‘˜ (Officers)  â”‚  åˆ†æå™¨ (Analyzers)  â”‚  å·¥ä½œæµ (Workflows)  â”‚
â”‚  â”œâ”€ Governor âœ…     â”‚  â”œâ”€ ColonyAnalyzerâœ… â”‚  â”œâ”€ CrisisManagement â”‚
â”‚  â”œâ”€ LogisticsOfficerâ”‚  â”œâ”€ SecurityAnalyzer â”‚  â””â”€ AutomationFlow   â”‚
â”‚  â””â”€ MilitaryOfficer â”‚  â””â”€ ThreatAnalyzer   â”‚                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    æ ¸å¿ƒæœåŠ¡å±‚ âœ… å®Œå…¨æ¢å¤                      â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚        â”‚   LLMService    â”‚  â”‚ ServiceContainer â”‚              â”‚
â”‚        â”‚(ILLMServiceå®ç°)â”‚  â”‚  + åˆ†æå™¨æœåŠ¡   â”‚ â† ğŸ¯ å·²æ¢å¤    â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                Framework Wrapper                           â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚              â”‚      RimAI.API          â”‚                    â”‚
â”‚              â”‚   (Framework å…¥å£)       â”‚                    â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                RimAI Framework Layer                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   LLM Services  â”‚  â”‚  Configuration  â”‚  â”‚   HTTP Client   â”‚ â”‚
â”‚  â”‚  â”œâ”€ OpenAI      â”‚  â”‚  â”œâ”€ Settings    â”‚  â”‚  â”œâ”€ Request     â”‚ â”‚
â”‚  â”‚  â”œâ”€ Claude      â”‚  â”‚  â”œâ”€ Providers   â”‚  â”‚  â”œâ”€ Response    â”‚ â”‚
â”‚  â”‚  â””â”€ Local       â”‚  â”‚  â””â”€ Security    â”‚  â”‚  â””â”€ Streaming   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### è°ƒç”¨æµç¨‹å›¾
```
ç”¨æˆ·äº¤äº’ â†’ UIç»„ä»¶ â†’ AIå®˜å‘˜ â†’ OfficerBase â†’ LLMService â†’ RimAIAPI â†’ Framework
   â†“         â†“        â†“         â†“          â†“           â†“          â†“
 æŒ‰é’®ç‚¹å‡»   è¯·æ±‚å»ºè®®   ä¸šåŠ¡é€»è¾‘   ç»Ÿä¸€å¤„ç†   æœåŠ¡å°è£…    APIè°ƒç”¨   å®é™…æ‰§è¡Œ
```

## ğŸ¯ æ ¸å¿ƒç»„ä»¶è¯´æ˜

### LLMService - Framework è°ƒç”¨çš„å”¯ä¸€å…¥å£

**ä½ç½®**: `RimAI.Core/Source/Services/LLMService.cs`

**èŒè´£**:
- å°è£…æ‰€æœ‰å¯¹ RimAI Framework çš„è°ƒç”¨
- æä¾›ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’ŒçŠ¶æ€ç®¡ç†
- æ”¯æŒæ ‡å‡†ã€JSON å’Œæµå¼è¯·æ±‚
- ç®¡ç† Framework çš„åˆå§‹åŒ–çŠ¶æ€

**å…³é”®ç‰¹æ€§**:
```csharp
public class LLMService : ILLMService
{
    // Framework çŠ¶æ€æ£€æŸ¥
    public bool IsInitialized => RimAIAPI.IsInitialized;
    public bool IsStreamingAvailable => RimAIAPI.IsStreamingEnabled;
    
    // æ ¸å¿ƒè°ƒç”¨æ–¹æ³•
    public async Task<string> SendMessageAsync(string prompt, LLMOptions options, CancellationToken cancellationToken)
    public async Task<T> SendJsonRequestAsync<T>(string prompt, LLMOptions options, CancellationToken cancellationToken) 
    public async Task SendStreamingMessageAsync(string prompt, Action<string> onChunk, LLMOptions options, CancellationToken cancellationToken)
}
```

## ğŸ“ å®é™…è°ƒç”¨ä»£ç è¿½è¸ª

### æ­¥éª¤1: ç”¨æˆ·è¯·æ±‚ AI å»ºè®®
```csharp
// UI å±‚è°ƒç”¨
var advice = await ResearchOfficer.Instance.GetAdviceAsync();
```

### æ­¥éª¤2: AI å®˜å‘˜å¤„ç† (ResearchOfficer)
```csharp
// ResearchOfficer ç»§æ‰¿è‡ª OfficerBase
// å®é™…è°ƒç”¨çˆ¶ç±»æ–¹æ³•
public override async Task<string> GetAdviceAsync(CancellationToken cancellationToken = default)
{
    return await base.GetAdviceAsync(cancellationToken); // è°ƒç”¨çˆ¶ç±»
}
```

### æ­¥éª¤3: åŸºç±»ç»Ÿä¸€å¤„ç† (OfficerBase)
```csharp
public virtual async Task<string> GetAdviceAsync(CancellationToken cancellationToken = default)
{
    // æ„å»ºä¸Šä¸‹æ–‡
    var context = await BuildContextAsync(cancellationToken);
    
    // æ„å»ºæç¤ºè¯
    var prompt = _promptBuilder.BuildPrompt(templateId, context);
    
    // ğŸ“ å…³é”®è°ƒç”¨ç‚¹ï¼šé€šè¿‡ LLMService è°ƒç”¨ Framework
    var response = await _llmService.SendMessageAsync(prompt, options, cancellationToken);
    
    return response;
}
```

### æ­¥éª¤4: LLMService åŒ…è£… Framework (å…³é”®ç»„ä»¶)
```csharp
public async Task<string> SendMessageAsync(string prompt, LLMOptions options = null, CancellationToken cancellationToken = default)
{
    if (!IsInitialized)
    {
        throw new InvalidOperationException("RimAI Framework is not initialized");
    }

    // ğŸ“ å®é™…è°ƒç”¨ Framework API
    var response = await RimAIAPI.SendMessageAsync(prompt, options, cancellationToken);
    return response ?? string.Empty;
}
```

### æ­¥éª¤5: Framework API æ‰§è¡Œ
```csharp
// RimAI.Framework.API.RimAIAPI
public static async Task<string> SendMessageAsync(string prompt, LLMOptions options = null, CancellationToken cancellationToken = default)
{
    // è¿™é‡Œæ˜¯ Framework çš„å…·ä½“å®ç°
    // åŒ…æ‹¬ï¼š
    // - HTTP è¯·æ±‚æ„å»º
    // - æä¾›å•†é€‰æ‹© (OpenAI/Claude/Local)
    // - è¯·æ±‚å‘é€
    // - å“åº”å¤„ç†
    // - é”™è¯¯å¤„ç†
    
    return await executor.ExecuteAsync(request);
}
```

## ğŸ” ä¸ºä»€ä¹ˆæ˜¯è¿™ç§è®¾è®¡ï¼Ÿ

### 1. **å•ä¸€è´£ä»»åŸåˆ™**
- LLMService ä¸“é—¨è´Ÿè´£ Framework è°ƒç”¨
- å…¶ä»–ç»„ä»¶ä¸“æ³¨äºä¸šåŠ¡é€»è¾‘

### 2. **ç»Ÿä¸€çš„é”™è¯¯å¤„ç†**
```csharp
// æ‰€æœ‰ Framework è°ƒç”¨éƒ½ç»è¿‡ç»Ÿä¸€çš„å¼‚å¸¸å¤„ç†
try
{
    var response = await RimAIAPI.SendMessageAsync(prompt, options, cancellationToken);
}
catch (Exception ex)
{
    Log.Error($"[LLMService] Request failed: {ex.Message}");
    throw;
}
```

### 3. **é…ç½®å’ŒçŠ¶æ€ç®¡ç†**
```csharp
public bool IsInitialized => RimAIAPI.IsInitialized;
public bool IsStreamingAvailable => RimAIAPI.IsStreamingEnabled;
```

### 4. **ä¾¿äºæµ‹è¯•å’Œæ¨¡æ‹Ÿ**
```csharp
// å¯ä»¥è½»æ¾æ›¿æ¢ LLMService è¿›è¡Œæµ‹è¯•
RegisterInstance<ILLMService>(MockLLMService.Instance);
```

## ğŸ¯ å…³é”®è¦ç‚¹

1. **LLMService æ˜¯å”¯ä¸€çš„ Framework è°ƒç”¨è€…**
2. **æ‰€æœ‰ AI åŠŸèƒ½éƒ½é€šè¿‡ LLMService é—´æ¥ä½¿ç”¨ Framework**
3. **è¿™ç§è®¾è®¡æä¾›äº†è§£è€¦ã€ç»Ÿä¸€ç®¡ç†å’Œé”™è¯¯å¤„ç†**
4. **Framework çš„å¤æ‚æ€§è¢«å®Œå…¨å°è£…åœ¨ LLMService ä¸­**

## ğŸ“‹ è°ƒç”¨ç»Ÿè®¡

åœ¨å½“å‰æ¶æ„ä¸­ï¼Œä»¥ä¸‹ç»„ä»¶ä¼šé—´æ¥ä½¿ç”¨ Frameworkï¼š

- âœ… **AI å®˜å‘˜** (é€šè¿‡ OfficerBase â†’ LLMService)
- âœ… **å·¥ä½œæµ** (ç›´æ¥è°ƒç”¨ LLMService)  
- âœ… **åˆ†æå™¨** (å¦‚æœéœ€è¦ AI åˆ†æï¼Œé€šè¿‡ LLMService)
- âœ… **UI ç»„ä»¶** (é€šè¿‡å®˜å‘˜æˆ–ç›´æ¥è°ƒç”¨ LLMService)

ä½†åªæœ‰ **LLMService** ç›´æ¥ä¸ Framework äº¤äº’ï¼

## ğŸ”§ é›†æˆå®ç°ç»†èŠ‚

### 1. æœåŠ¡æ³¨å†Œä¸ä¾èµ–æ³¨å…¥

**ServiceContainer æ³¨å†Œ**:
```csharp
// RimAI.Core/Source/Architecture/ServiceContainer.cs
private void RegisterDefaultServices()
{
    // æ ¸å¿ƒæœåŠ¡
    RegisterInstance<ILLMService>(LLMService.Instance);
    RegisterInstance<IColonyAnalyzer>(ColonyAnalyzer.Instance);
    RegisterInstance<IPromptBuilder>(PromptBuilder.Instance);
    // ... å…¶ä»–æœåŠ¡
}
```

**ä¾èµ–æ³¨å…¥ä½¿ç”¨**:
```csharp
// åœ¨ OfficerBase ä¸­
protected OfficerBase()
{
    _llmService = LLMService.Instance;  // è·å– Framework è°ƒç”¨è€…
    _promptBuilder = PromptBuilder.Instance;
    _analyzer = ColonyAnalyzer.Instance;
    _cacheService = CacheService.Instance;
}
```

### 2. æ¥å£è®¾è®¡æ¨¡å¼

**ILLMService æ¥å£**:
```csharp
public interface ILLMService
{
    Task<string> SendMessageAsync(string prompt, LLMOptions options = null, CancellationToken cancellationToken = default);
    Task<T> SendJsonRequestAsync<T>(string prompt, LLMOptions options = null, CancellationToken cancellationToken = default) where T : class;
    Task SendStreamingMessageAsync(string prompt, Action<string> onChunk, LLMOptions options = null, CancellationToken cancellationToken = default);
    bool IsStreamingAvailable { get; }
    bool IsInitialized { get; }
}
```

### 3. é”™è¯¯å¤„ç†ç­–ç•¥

**ç»Ÿä¸€å¼‚å¸¸å¤„ç†**:
```csharp
public async Task<string> SendMessageAsync(string prompt, LLMOptions options = null, CancellationToken cancellationToken = default)
{
    if (!IsInitialized)
    {
        throw new InvalidOperationException("RimAI Framework is not initialized");
    }

    try
    {
        var response = await RimAIAPI.SendMessageAsync(prompt, options, cancellationToken);
        return response ?? string.Empty;
    }
    catch (OperationCanceledException)
    {
        Log.Message("[LLMService] Request was cancelled");
        throw;
    }
    catch (Exception ex)
    {
        Log.Error($"[LLMService] Request failed: {ex.Message}");
        throw;
    }
}
```

### 4. ç¼“å­˜ç­–ç•¥é›†æˆ

**å®˜å‘˜çº§åˆ«ç¼“å­˜**:
```csharp
public virtual async Task<string> GetAdviceAsync(CancellationToken cancellationToken = default)
{
    var cacheKey = GenerateCacheKey("advice");
    
    return await _cacheService.GetOrCreateAsync(
        cacheKey,
        () => ExecuteAdviceRequest(cancellationToken),
        TimeSpan.FromMinutes(2) // å»ºè®®ç¼“å­˜2åˆ†é’Ÿ
    );
}
```

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€è°ƒç”¨ç¤ºä¾‹
```csharp
// åœ¨ UI æˆ–å…¶ä»–ç»„ä»¶ä¸­
public async void OnGetAdviceButtonClick()
{
    try
    {
        // é€šè¿‡å®˜å‘˜è·å–å»ºè®®ï¼ˆæ¨èæ–¹å¼ï¼‰
        var advice = await ResearchOfficer.Instance.GetAdviceAsync();
        DisplayAdvice(advice);
    }
    catch (InvalidOperationException ex)
    {
        ShowError($"æœåŠ¡æœªå°±ç»ª: {ex.Message}");
    }
    catch (Exception ex)
    {
        ShowError($"è·å–å»ºè®®å¤±è´¥: {ex.Message}");
    }
}
```

### ç›´æ¥æœåŠ¡è°ƒç”¨ç¤ºä¾‹
```csharp
// ç›´æ¥ä½¿ç”¨ LLMServiceï¼ˆé«˜çº§ç”¨æ³•ï¼‰
public async Task<string> GetCustomAnalysis(string contextData)
{
    var llmService = ServiceContainer.Instance.GetService<ILLMService>();
    
    if (!llmService.IsInitialized)
    {
        return "AIæœåŠ¡æœªåˆå§‹åŒ–";
    }

    var prompt = $@"è¯·åˆ†æä»¥ä¸‹æ®–æ°‘åœ°æ•°æ®ï¼š
{contextData}

æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®ã€‚";

    try
    {
        var options = new LLMOptions { Temperature = 0.7f };
        return await llmService.SendMessageAsync(prompt, options);
    }
    catch (Exception ex)
    {
        Log.Error($"Analysis failed: {ex.Message}");
        return $"åˆ†æå¤±è´¥: {ex.Message}";
    }
}
```

### æµå¼å“åº”ç¤ºä¾‹
```csharp
public async Task GetStreamingAdvice(Action<string> onPartialResponse)
{
    var llmService = LLMService.Instance;
    
    if (!llmService.IsStreamingAvailable)
    {
        onPartialResponse("æµå¼å“åº”ä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†å“åº”...");
        var result = await llmService.SendMessageAsync(prompt);
        onPartialResponse(result);
        return;
    }

    await llmService.SendStreamingMessageAsync(
        prompt, 
        chunk => onPartialResponse(chunk),  // å®æ—¶æ˜¾ç¤ºéƒ¨åˆ†å“åº”
        options
    );
}
```

## âš™ï¸ é…ç½®ç®¡ç†

### Framework åˆå§‹åŒ–æ£€æŸ¥
```csharp
public static class FrameworkStatus
{
    public static bool IsReady => LLMService.Instance.IsInitialized;
    
    public static string GetStatusInfo()
    {
        var service = LLMService.Instance;
        return $@"FrameworkçŠ¶æ€:
- å·²åˆå§‹åŒ–: {service.IsInitialized}
- æµå¼æ”¯æŒ: {service.IsStreamingAvailable}
- å½“å‰è®¾ç½®: {service.GetCurrentSettings()}";
    }
}
```

### è¿æ¥æµ‹è¯•
```csharp
public async Task<bool> TestFrameworkConnection()
{
    try
    {
        var (success, message) = await LLMService.Instance.TestConnectionAsync();
        
        if (success)
        {
            Messages.Message("Framework è¿æ¥æ­£å¸¸", MessageTypeDefOf.PositiveEvent);
        }
        else
        {
            Messages.Message($"Framework è¿æ¥å¤±è´¥: {message}", MessageTypeDefOf.RejectInput);
        }
        
        return success;
    }
    catch (Exception ex)
    {
        Log.Error($"Connection test failed: {ex.Message}");
        return false;
    }
}
```

## ğŸ§ª æµ‹è¯•ä¸è°ƒè¯•

### Mock LLMService ç”¨äºæµ‹è¯•
```csharp
public class MockLLMService : ILLMService
{
    public bool IsInitialized => true;
    public bool IsStreamingAvailable => false;

    public async Task<string> SendMessageAsync(string prompt, LLMOptions options = null, CancellationToken cancellationToken = default)
    {
        // æ¨¡æ‹Ÿå»¶è¿Ÿ
        await Task.Delay(100, cancellationToken);
        
        // è¿”å›æµ‹è¯•å“åº”
        return $"Mock response for: {prompt.Substring(0, Math.Min(50, prompt.Length))}...";
    }

    public async Task<T> SendJsonRequestAsync<T>(string prompt, LLMOptions options = null, CancellationToken cancellationToken = default) where T : class
    {
        // è¿”å›é»˜è®¤å®ä¾‹æˆ–æµ‹è¯•æ•°æ®
        return Activator.CreateInstance<T>();
    }

    public async Task SendStreamingMessageAsync(string prompt, Action<string> onChunk, LLMOptions options = null, CancellationToken cancellationToken = default)
    {
        var mockResponse = "This is a mock streaming response.";
        var words = mockResponse.Split(' ');
        
        foreach (var word in words)
        {
            onChunk?.Invoke(word + " ");
            await Task.Delay(50, cancellationToken);
        }
    }
}

// åœ¨æµ‹è¯•ä¸­ä½¿ç”¨
ServiceContainer.Instance.RegisterInstance<ILLMService>(new MockLLMService());
```

### è°ƒè¯•æ—¥å¿—é…ç½®
```csharp
// å¯ç”¨è¯¦ç»†æ—¥å¿—è®°å½•
public class VerboseLLMService : LLMService
{
    public override async Task<string> SendMessageAsync(string prompt, LLMOptions options = null, CancellationToken cancellationToken = default)
    {
        Log.Message($"[Debug] Sending prompt: {prompt.Substring(0, Math.Min(100, prompt.Length))}...");
        Log.Message($"[Debug] Options: {options?.ToString() ?? "null"}");
        
        var stopwatch = System.Diagnostics.Stopwatch.StartNew();
        var result = await base.SendMessageAsync(prompt, options, cancellationToken);
        stopwatch.Stop();
        
        Log.Message($"[Debug] Response received in {stopwatch.ElapsedMilliseconds}ms, length: {result?.Length ?? 0}");
        
        return result;
    }
}
```

## ğŸ”§ é«˜çº§é›†æˆæ¨¡å¼

### å¼‚æ­¥é˜Ÿåˆ—å¤„ç†
```csharp
public class QueuedLLMService
{
    private readonly Queue<LLMRequest> _requestQueue = new Queue<LLMRequest>();
    private readonly SemaphoreSlim _semaphore = new SemaphoreSlim(1, 1);
    
    public async Task<string> QueuedSendMessageAsync(string prompt, LLMOptions options = null)
    {
        var request = new LLMRequest { Prompt = prompt, Options = options };
        var tcs = new TaskCompletionSource<string>();
        request.CompletionSource = tcs;
        
        await _semaphore.WaitAsync();
        try
        {
            _requestQueue.Enqueue(request);
            ProcessQueueAsync(); // ä¸ç­‰å¾…ï¼Œå¼‚æ­¥å¤„ç†
        }
        finally
        {
            _semaphore.Release();
        }
        
        return await tcs.Task;
    }
}
```

### æ‰¹é‡è¯·æ±‚å¤„ç†
```csharp
public async Task<List<string>> SendBatchMessagesAsync(List<string> prompts, LLMOptions options = null)
{
    var tasks = prompts.Select(prompt => 
        LLMService.Instance.SendMessageAsync(prompt, options)
    ).ToArray();
    
    var results = await Task.WhenAll(tasks);
    return results.ToList();
}
```

## ğŸ“š æœ€ä½³å®è·µ

### 1. é”™è¯¯æ¢å¤ç­–ç•¥
```csharp
public async Task<string> GetAdviceWithRetry(string prompt, int maxRetries = 3)
{
    for (int attempt = 1; attempt <= maxRetries; attempt++)
    {
        try
        {
            return await LLMService.Instance.SendMessageAsync(prompt);
        }
        catch (Exception ex) when (attempt < maxRetries)
        {
            Log.Warning($"Attempt {attempt} failed: {ex.Message}, retrying...");
            await Task.Delay(TimeSpan.FromSeconds(Math.Pow(2, attempt))); // æŒ‡æ•°é€€é¿
        }
    }
    
    throw new Exception($"Failed after {maxRetries} attempts");
}
```

### 2. èµ„æºç®¡ç†
```csharp
public async Task<string> GetAdviceWithTimeout(string prompt, TimeSpan timeout)
{
    using var cts = new CancellationTokenSource(timeout);
    
    try
    {
        return await LLMService.Instance.SendMessageAsync(prompt, null, cts.Token);
    }
    catch (OperationCanceledException) when (cts.Token.IsCancellationRequested)
    {
        throw new TimeoutException($"Request timed out after {timeout.TotalSeconds} seconds");
    }
}
```

### 3. æ€§èƒ½ç›‘æ§
```csharp
public class PerformanceMonitoredLLMService : ILLMService
{
    private readonly ILLMService _inner = LLMService.Instance;
    private readonly Dictionary<string, long> _performanceMetrics = new Dictionary<string, long>();
    
    public async Task<string> SendMessageAsync(string prompt, LLMOptions options = null, CancellationToken cancellationToken = default)
    {
        var stopwatch = Stopwatch.StartNew();
        try
        {
            var result = await _inner.SendMessageAsync(prompt, options, cancellationToken);
            return result;
        }
        finally
        {
            stopwatch.Stop();
            RecordMetric("SendMessage", stopwatch.ElapsedMilliseconds);
        }
    }
    
    private void RecordMetric(string operation, long milliseconds)
    {
        var key = $"{operation}_avg_ms";
        _performanceMetrics[key] = (_performanceMetrics.GetValueOrDefault(key) + milliseconds) / 2;
    }
}
```

## ğŸ¯ é›†æˆæ£€æŸ¥æ¸…å•

- [ ] **LLMService æ­£ç¡®å®ç° ILLMService æ¥å£**
- [ ] **æ‰€æœ‰ Framework è°ƒç”¨éƒ½ç»è¿‡ LLMService**
- [ ] **é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•å®Œæ•´**
- [ ] **æ”¯æŒå–æ¶ˆä»¤ç‰Œå’Œè¶…æ—¶å¤„ç†**
- [ ] **Framework åˆå§‹åŒ–çŠ¶æ€æ£€æŸ¥**
- [ ] **ç¼“å­˜ç­–ç•¥æ­£ç¡®é…ç½®**
- [ ] **å•å…ƒæµ‹è¯•è¦†ç›– Mock æœåŠ¡**
- [ ] **æ€§èƒ½ç›‘æ§å’Œè°ƒè¯•æ”¯æŒ**
- [ ] **å¼‚å¸¸æƒ…å†µä¸‹çš„ä¼˜é›…é™çº§**
- [ ] **èµ„æºæ¸…ç†å’Œå†…å­˜ç®¡ç†**

é€šè¿‡è¿™ç§æ¶æ„è®¾è®¡ï¼ŒRimAI Core å®ç°äº†ä¸ Framework çš„æ¾è€¦åˆé›†æˆï¼Œæä¾›äº†å¼ºå¤§è€Œçµæ´»çš„ AI åŠŸèƒ½ï¼
